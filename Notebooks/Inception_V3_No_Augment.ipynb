{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## ğŸŒ Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "2e611df2-844b-415b-f96b-dc5f30a5ba49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive\n",
      "/gdrive/My Drive/[2024-2025] AN2DL Homework 1\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/\n",
    "%cd [2024-2025] AN2DL Homework 1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRQSldIb4i1V"
   },
   "source": [
    "## ğŸ›  Fix Codabench Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-9WtuCR4iQk",
    "outputId": "4d63bf35-c71f-4e55-ad74-11b979275253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Creates a file in which we specify the versions of the libraries we want\n",
    "%%writefile requirements.txt\n",
    "tensorflow==2.17.0\n",
    "keras==3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lsNAwawq8PNr",
    "outputId": "8ef39bf0-7a4d-4a50-baa0-97b3100a9661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.0 (from -r requirements.txt (line 1))\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting keras==3.4.1 (from -r requirements.txt (line 2))\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.45.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.4.1->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.1\n",
      "    Uninstalling tensorflow-2.17.1:\n",
      "      Successfully uninstalled tensorflow-2.17.1\n",
      "Successfully installed keras-3.4.1 tensorflow-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## âš™ï¸ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CO6_Ft_8T56A",
    "outputId": "34f7e344-48a5-471d-e420-83187a2ddaac"
   },
   "outputs": [   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seeds for NumPy and TensorFlow\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed);\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## â³ Load and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLaoDaG1V1Yg",
    "outputId": "b3acba1c-359e-434c-8425-315c2d4c5a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape:  (13759, 96, 96, 3)\n",
      "Initial y shape:  (13759, 1)\n",
      "Final X shape:  (11959, 96, 96, 3)\n",
      "Final y shape:  (11959, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = np.load('training_set.npz')\n",
    "\n",
    "# Put images on X and labels on y\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "print(\"Initial X shape: \", X.shape)\n",
    "print(\"Initial y shape: \", y.shape)\n",
    "\n",
    "# Delete outliers from the dataset\n",
    "delete_index = 11958\n",
    "X = X[:delete_index + 1]\n",
    "y = y[:delete_index + 1]\n",
    "\n",
    "print(\"Final X shape: \", X.shape)\n",
    "print(\"Final y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5a0oGBH_hjd"
   },
   "source": [
    "## ğŸš† Split into train, validation and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcKedqJg__oK",
    "outputId": "315ac134-d21b-461e-cad1-d1ebba304363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:\t (9567, 96, 96, 3) (9567, 8)\n",
      "Validation set shape:\t (1196, 96, 96, 3) (1196, 8)\n",
      "Test set shape:\t\t (1196, 96, 96, 3) (1196, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a training + validation set, and a separate test set\n",
    "# The test set is the 10% of the whole dataset\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    stratify=y,\n",
    "    random_state=seed)\n",
    "\n",
    "# Further split the training + validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=len(X_test),\n",
    "    stratify=y_train_val,\n",
    "    random_state=seed)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tfk.utils.to_categorical(y_train, 8)\n",
    "y_val = tfk.utils.to_categorical(y_val, 8)\n",
    "y_test = tfk.utils.to_categorical(y_test, 8)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
    "print('Validation set shape:\\t', X_val.shape, y_val.shape)\n",
    "print('Test set shape:\\t\\t', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW5huZxzGInt"
   },
   "source": [
    "## ğŸ§® Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q42LjE5QGKc_"
   },
   "outputs": [],
   "source": [
    "# Input shape for the model\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 100\n",
    "\n",
    "# Number of samples passed to the network at each training step\n",
    "batch_size = 16\n",
    "\n",
    "# Learning rate: step size for updating the model's weights\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# L2 Lambda for regularization\n",
    "l2_lambda = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7rRsq2pXQfH"
   },
   "source": [
    "## ğŸ”¨ Import and tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94n95sZ4XTtY",
    "outputId": "dfce1e10-8ff6-42dc-be87-bfd45070bb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize InceptionV3\n",
    "model = tfk.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=input_shape,\n",
    "    pooling='avg',\n",
    "    classes=output_shape,\n",
    "    classifier_activation='softmax',\n",
    ")\n",
    "\n",
    "# Initialize regularizer\n",
    "regularizer = tfk.regularizers.L2(l2_lambda)\n",
    "\n",
    "# Freeze all layers to use the model solely as a feature extractor\n",
    "model.trainable = False\n",
    "\n",
    "# Create input layer\n",
    "inputs = tfkl.Input(shape=input_shape)\n",
    "\n",
    "# Connect model with inputs\n",
    "x = model(inputs, training=False)\n",
    "\n",
    "# Add layers\n",
    "x = tfkl.Dense(1024, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "x = tfkl.BatchNormalization()(x)\n",
    "x = tfkl.Dropout(0.5)(x)\n",
    "x = tfkl.Dense(1024, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "x = tfkl.BatchNormalization()(x)\n",
    "x = tfkl.Dropout(0.5)(x)\n",
    "\n",
    "# Setup Fully Connected Blocks\n",
    "x = tfkl.Dropout(rate=0.3)(x)\n",
    "outputs = tfkl.Dense(units=output_shape, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "# Connect input and output\n",
    "model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "loss = tfk.losses.CategoricalCrossentropy()\n",
    "optimizer = tfk.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## ğŸ§  Train the Model for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5sr8CzivVdvI"
   },
   "outputs": [],
   "source": [
    "# Create an EarlyStopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Create a LearningRate Scheduler, which reduces learning rate if val_loss doesn't improve\n",
    "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ox9jqYyyUJo0",
    "outputId": "ea7b34b5-0d1a-47ed-ebd2-fcb66f8c4e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 45ms/step - accuracy: 0.2531 - loss: 2.9836 - val_accuracy: 0.5920 - val_loss: 1.2311 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.4230 - loss: 2.1057 - val_accuracy: 0.6455 - val_loss: 1.1394 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4849 - loss: 1.8677 - val_accuracy: 0.6564 - val_loss: 1.1040 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5104 - loss: 1.8008 - val_accuracy: 0.6672 - val_loss: 1.0625 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5370 - loss: 1.6657 - val_accuracy: 0.6831 - val_loss: 1.0201 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.5489 - loss: 1.6251 - val_accuracy: 0.6873 - val_loss: 1.0086 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.5592 - loss: 1.5762 - val_accuracy: 0.6915 - val_loss: 0.9931 - learning_rate: 1.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.5664 - loss: 1.5719 - val_accuracy: 0.6915 - val_loss: 0.9818 - learning_rate: 1.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.5799 - loss: 1.5111 - val_accuracy: 0.6898 - val_loss: 0.9701 - learning_rate: 1.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.5964 - loss: 1.4552 - val_accuracy: 0.7023 - val_loss: 0.9565 - learning_rate: 1.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.5919 - loss: 1.4250 - val_accuracy: 0.6998 - val_loss: 0.9393 - learning_rate: 1.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.6005 - loss: 1.4309 - val_accuracy: 0.6990 - val_loss: 0.9336 - learning_rate: 1.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.6071 - loss: 1.3861 - val_accuracy: 0.7032 - val_loss: 0.9223 - learning_rate: 1.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6030 - loss: 1.3888 - val_accuracy: 0.7065 - val_loss: 0.9156 - learning_rate: 1.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6039 - loss: 1.3931 - val_accuracy: 0.7124 - val_loss: 0.9083 - learning_rate: 1.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6088 - loss: 1.3546 - val_accuracy: 0.7182 - val_loss: 0.9021 - learning_rate: 1.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6125 - loss: 1.3279 - val_accuracy: 0.7107 - val_loss: 0.9041 - learning_rate: 1.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.6193 - loss: 1.3253 - val_accuracy: 0.7224 - val_loss: 0.8751 - learning_rate: 1.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.6267 - loss: 1.3169 - val_accuracy: 0.7283 - val_loss: 0.8703 - learning_rate: 1.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6142 - loss: 1.3017 - val_accuracy: 0.7241 - val_loss: 0.8609 - learning_rate: 1.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6244 - loss: 1.2923 - val_accuracy: 0.7224 - val_loss: 0.8526 - learning_rate: 1.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6405 - loss: 1.2158 - val_accuracy: 0.7241 - val_loss: 0.8471 - learning_rate: 1.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6338 - loss: 1.2457 - val_accuracy: 0.7224 - val_loss: 0.8371 - learning_rate: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.6299 - loss: 1.2309 - val_accuracy: 0.7333 - val_loss: 0.8349 - learning_rate: 1.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6329 - loss: 1.2396 - val_accuracy: 0.7391 - val_loss: 0.8494 - learning_rate: 1.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6389 - loss: 1.1910 - val_accuracy: 0.7324 - val_loss: 0.8403 - learning_rate: 1.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6374 - loss: 1.2197 - val_accuracy: 0.7299 - val_loss: 0.8343 - learning_rate: 1.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6368 - loss: 1.1970 - val_accuracy: 0.7333 - val_loss: 0.8365 - learning_rate: 1.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.6508 - loss: 1.1644 - val_accuracy: 0.7458 - val_loss: 0.8296 - learning_rate: 1.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6465 - loss: 1.1523 - val_accuracy: 0.7375 - val_loss: 0.8227 - learning_rate: 1.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6456 - loss: 1.1767 - val_accuracy: 0.7383 - val_loss: 0.8218 - learning_rate: 1.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.6402 - loss: 1.1834 - val_accuracy: 0.7425 - val_loss: 0.8164 - learning_rate: 1.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.6536 - loss: 1.1645 - val_accuracy: 0.7425 - val_loss: 0.8168 - learning_rate: 1.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.6539 - loss: 1.1510 - val_accuracy: 0.7450 - val_loss: 0.8026 - learning_rate: 1.0000e-05\n",
      "Training finished.\n",
      "Final validation accuracy: 74.58%\n"
     ]
    }
   ],
   "source": [
    "tl_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "tl_final_val_accuracy = round(max(tl_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {tl_final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "tl_model_filename = 'InceptionV3TL' + str(tl_final_val_accuracy) + '.keras'\n",
    "model.save(tl_model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJfYvaTyenL9"
   },
   "source": [
    "## ğŸ”§ Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vd2tLbkueuTt"
   },
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model(tl_model_filename)\n",
    "\n",
    "# Set the model layers as trainable\n",
    "ft_model.get_layer('inception_v3').trainable = True\n",
    "\n",
    "# Set all layers as non-trainable\n",
    "for layer in ft_model.get_layer('inception_v3').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "t3N_PBk2gqiR"
   },
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 200\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "loss = tfk.losses.CategoricalCrossentropy()\n",
    "optimizer = tfk.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy']\n",
    "ft_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc_dOYkghI92"
   },
   "source": [
    "## ğŸ§  Train Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmWtYFf7hHLc",
    "outputId": "a83bd335-4a39-4d30-efc6-106ad0d27c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.3587 - loss: 2.1949 - val_accuracy: 0.1814 - val_loss: 3.6110 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.3336 - loss: 2.2158 - val_accuracy: 0.3052 - val_loss: 3.1108 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.3730 - loss: 2.0321 - val_accuracy: 0.4691 - val_loss: 1.5152 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.3433 - loss: 2.1875 - val_accuracy: 0.2550 - val_loss: 2.1989 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.3777 - loss: 2.0609 - val_accuracy: 0.3378 - val_loss: 2.2308 - learning_rate: 1.0000e-05\n",
      "Training finished.\n",
      "Final validation accuracy: 46.91%\n"
     ]
    }
   ],
   "source": [
    "ft_history = ft_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "ft_final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {ft_final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file\n",
    "ft_model.save('weights.keras')\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YES9mYNvzdSo"
   },
   "source": [
    "## âœ… Verify that the weights work as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDrXpqfUzexe",
    "outputId": "1ebb78a2-faac-4bc3-e319-feba69904aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Validation Accuracy: 0.1814\n",
      "Test Accuracy: 0.1747\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = tfk.models.load_model('weights.keras')\n",
    "\n",
    "# Predict on test set and validation set\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Convert to class labels\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "y_pred_val_classes = np.argmax(y_pred_val, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "y_val_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "test_accuracy = np.sum(y_test_classes == y_pred_test_classes) / len(y_test_classes)\n",
    "val_accuracy = np.sum(y_val_classes == y_pred_val_classes) / len(y_val_classes)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## ğŸ“Š Create the model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKT4h-9xYwiT",
    "outputId": "f6290103-2948-4f93-d30f-0b38a4ef50a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.neural_network = tfk.models.load_model('weights.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CircvnFi4nX_"
   },
   "source": [
    "## ğŸ“ Export the ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s18kX1uDconq",
    "outputId": "f98ccb36-4980-473e-bd0a-76fff6e22aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.py (deflated 49%)\n",
      "  adding: weights.keras (deflated 8%)\n"
     ]
    }
   ],
   "source": [
    "# Set filename for the zip file\n",
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Create a zip file with the provided filename, containing model and weights\n",
    "!zip {filename} model.py weights.keras"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
