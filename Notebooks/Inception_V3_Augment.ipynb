{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## ğŸŒ Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "2cbf914f-667e-4278-9f57-e473c6a58bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive\n",
      "/gdrive/My Drive/[2024-2025] AN2DL Homework 1\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/\n",
    "%cd [2024-2025] AN2DL Homework 1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRQSldIb4i1V"
   },
   "source": [
    "## ğŸ›  Fix Codabench Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-9WtuCR4iQk",
    "outputId": "4d63bf35-c71f-4e55-ad74-11b979275253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Creates a file in which we specify the versions of the libraries we want\n",
    "%%writefile requirements.txt\n",
    "tensorflow==2.17.0\n",
    "keras==3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lsNAwawq8PNr",
    "outputId": "8eb07752-90d2-4076-9c33-32ee679e0e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.0 (from -r requirements.txt (line 1))\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting keras==3.4.1 (from -r requirements.txt (line 2))\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.45.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.4.1->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.1\n",
      "    Uninstalling tensorflow-2.17.1:\n",
      "      Successfully uninstalled tensorflow-2.17.1\n",
      "Successfully installed keras-3.4.1 tensorflow-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## âš™ï¸ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CO6_Ft_8T56A",
    "outputId": "69550565-a03d-48d1-f0d4-61ff45ff0b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-cv\n",
      "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (24.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2024.9.11)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.7)\n",
      "Collecting keras-core (from keras-cv)\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-cv) (0.3.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (4.66.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (3.12.1)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.2.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.25.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.13.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.5.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.16.0)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.5.1)\n",
      "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (6.4.5)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2024.8.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.18.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras-cv) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.66.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
      "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
      "Successfully installed keras-core-0.1.7 keras-cv-0.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "!pip install keras-cv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_cv as kcv\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seeds for NumPy and TensorFlow\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed);\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## â³ Load and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLaoDaG1V1Yg",
    "outputId": "81f2ec38-4b35-49ce-94d6-c4dd6be83d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape:  (13759, 96, 96, 3)\n",
      "Initial y shape:  (13759, 1)\n",
      "Final X shape:  (11959, 96, 96, 3)\n",
      "Final y shape:  (11959, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = np.load('training_set.npz')\n",
    "\n",
    "# Put images on X and labels on y\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "print(\"Initial X shape: \", X.shape)\n",
    "print(\"Initial y shape: \", y.shape)\n",
    "\n",
    "# Delete outliers from the dataset\n",
    "delete_index = 11958\n",
    "X = X[:delete_index + 1]\n",
    "y = y[:delete_index + 1]\n",
    "\n",
    "print(\"Final X shape: \", X.shape)\n",
    "print(\"Final y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5a0oGBH_hjd"
   },
   "source": [
    "## ğŸš† Split into train, validation and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcKedqJg__oK",
    "outputId": "3e0c3c3c-1123-4019-87d3-0bc63d4b5639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:\t (9567, 96, 96, 3) (9567, 8)\n",
      "Validation set shape:\t (1196, 96, 96, 3) (1196, 8)\n",
      "Test set shape:\t\t (1196, 96, 96, 3) (1196, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a training + validation set, and a separate test set\n",
    "# The test set is the 10% of the whole dataset\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    stratify=y,\n",
    "    random_state=seed)\n",
    "\n",
    "# Further split the training + validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=len(X_test),\n",
    "    stratify=y_train_val,\n",
    "    random_state=seed)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tfk.utils.to_categorical(y_train, 8)\n",
    "y_val = tfk.utils.to_categorical(y_val, 8)\n",
    "y_test = tfk.utils.to_categorical(y_test, 8)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
    "print('Validation set shape:\\t', X_val.shape, y_val.shape)\n",
    "print('Test set shape:\\t\\t', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW5huZxzGInt"
   },
   "source": [
    "## ğŸ§® Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q42LjE5QGKc_"
   },
   "outputs": [],
   "source": [
    "# Input shape for the model\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 100\n",
    "\n",
    "# Number of samples passed to the network at each training step\n",
    "batch_size = 16\n",
    "\n",
    "# Learning rate: step size for updating the model's weights\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# L2 Lambda for regularization\n",
    "l2_lambda = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "193HtGOK52cA"
   },
   "source": [
    "## ğŸ†™  Define Image Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mcA_M6Uo5949"
   },
   "outputs": [],
   "source": [
    "augmix = kcv.layers.AugMix(\n",
    "    value_range=(0, 255),\n",
    "    severity=0.1,\n",
    "    num_chains=1,\n",
    "    chain_depth=[1, 2],\n",
    "    alpha=0.5,\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "gridmask = kcv.layers.GridMask(\n",
    "    ratio_factor=(0, 0.3),\n",
    "    rotation_factor=0.1,\n",
    "    fill_mode=\"constant\",\n",
    "    fill_value=0.0,\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "randaugment = kcv.layers.RandAugment(\n",
    "    value_range=(0, 255),\n",
    "    augmentations_per_image=1,\n",
    "    magnitude=0.2,\n",
    "    magnitude_stddev=0.1,\n",
    "    rate=0.3,\n",
    "    geometric=False,\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "channel_shuffle = kcv.layers.ChannelShuffle(\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "random_hue = kcv.layers.RandomHue(\n",
    "    factor=(0, 0.05),\n",
    "    value_range=(0, 255),\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "random_shear = kcv.layers.RandomShear(\n",
    "    x_factor=(0, 0.3),\n",
    "    y_factor=(0, 0.3),\n",
    "    interpolation=\"bilinear\",\n",
    "    fill_mode=\"constant\",\n",
    "    fill_value=0.0,\n",
    "    dtype=\"float32\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "random_flip = tfk.layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "random_rotation = tfk.layers.RandomRotation(0.2)\n",
    "\n",
    "augmentation = tfk.Sequential([\n",
    "    augmix,\n",
    "    gridmask,\n",
    "    randaugment,\n",
    "    channel_shuffle,\n",
    "    random_hue,\n",
    "    random_shear,\n",
    "    random_flip,\n",
    "    random_rotation,\n",
    "])\n",
    "\n",
    "# Cast the dtype to float32 to apply Augmentation Pipeline\n",
    "tfk.mixed_precision.set_global_policy('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBLCQo7EK1cZ"
   },
   "source": [
    "## â¯ Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5zRTVLLJ6hW"
   },
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "def preprocess_image(image):\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    return image\n",
    "\n",
    "def apply_augmentation(image, label):\n",
    "    image = augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "# Create the training dataset with augmentation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.map(lambda img, label: (preprocess_image(img), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=12000, seed=seed)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the validation dataset without augmentation\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.map(lambda img, label: (preprocess_image(img), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7rRsq2pXQfH"
   },
   "source": [
    "## ğŸ”¨ Import and tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "94n95sZ4XTtY"
   },
   "outputs": [],
   "source": [
    "# Import and initialize InceptionV3\n",
    "model = tfk.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=input_shape,\n",
    "    pooling='avg',\n",
    "    classes=output_shape,\n",
    "    classifier_activation='softmax',\n",
    ")\n",
    "\n",
    "# Initialize regularizer\n",
    "regularizer = tfk.regularizers.L2(l2_lambda)\n",
    "\n",
    "# Freeze all layers to use the model solely as a feature extractor\n",
    "model.trainable = False\n",
    "\n",
    "# Create input layer\n",
    "inputs = tfkl.Input(shape=input_shape)\n",
    "\n",
    "# Connect model with inputs\n",
    "x = model(inputs, training=False)\n",
    "\n",
    "# Add layers\n",
    "x = tfkl.Dense(1024, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "x = tfkl.BatchNormalization()(x)\n",
    "x = tfkl.Dropout(0.5)(x)\n",
    "x = tfkl.Dense(1024, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "x = tfkl.BatchNormalization()(x)\n",
    "x = tfkl.Dropout(0.5)(x)\n",
    "\n",
    "# Setup Fully Connected Blocks\n",
    "x = tfkl.Dropout(rate=0.3)(x)\n",
    "outputs = tfkl.Dense(units=output_shape, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "# Connect input and output\n",
    "model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "loss = tfk.losses.CategoricalCrossentropy()\n",
    "optimizer = tfk.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## ğŸ§  Train the Model for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sr8CzivVdvI"
   },
   "outputs": [],
   "source": [
    "# Create an EarlyStopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Create a LearningRate Scheduler, which reduces learning rate if val_loss doesn't improve\n",
    "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ox9jqYyyUJo0",
    "outputId": "f1511516-0703-4664-8f9b-d1efadb2e7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 50ms/step - accuracy: 0.1352 - loss: 3.8090 - val_accuracy: 0.4147 - val_loss: 1.7049 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 17ms/step - accuracy: 0.1715 - loss: 3.4179 - val_accuracy: 0.4649 - val_loss: 1.5501 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 18ms/step - accuracy: 0.2049 - loss: 3.2836 - val_accuracy: 0.5184 - val_loss: 1.3996 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 16ms/step - accuracy: 0.2102 - loss: 3.1964 - val_accuracy: 0.5510 - val_loss: 1.3346 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 16ms/step - accuracy: 0.2066 - loss: 3.1476 - val_accuracy: 0.5585 - val_loss: 1.3299 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 17ms/step - accuracy: 0.2254 - loss: 3.0753 - val_accuracy: 0.5644 - val_loss: 1.2915 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.2251 - loss: 3.0525 - val_accuracy: 0.5694 - val_loss: 1.2319 - learning_rate: 1.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 17ms/step - accuracy: 0.2352 - loss: 3.0014 - val_accuracy: 0.5819 - val_loss: 1.2357 - learning_rate: 1.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.2293 - loss: 2.9968 - val_accuracy: 0.5836 - val_loss: 1.2103 - learning_rate: 1.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.2396 - loss: 2.9113 - val_accuracy: 0.5861 - val_loss: 1.2039 - learning_rate: 1.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.2414 - loss: 2.8859 - val_accuracy: 0.5903 - val_loss: 1.1938 - learning_rate: 1.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 17ms/step - accuracy: 0.2509 - loss: 2.8184 - val_accuracy: 0.5711 - val_loss: 1.2293 - learning_rate: 1.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 17ms/step - accuracy: 0.2515 - loss: 2.8538 - val_accuracy: 0.5886 - val_loss: 1.1850 - learning_rate: 1.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 17ms/step - accuracy: 0.2365 - loss: 2.7964 - val_accuracy: 0.5719 - val_loss: 1.2068 - learning_rate: 1.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.2438 - loss: 2.8069 - val_accuracy: 0.5803 - val_loss: 1.1976 - learning_rate: 1.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 18ms/step - accuracy: 0.2427 - loss: 2.7452 - val_accuracy: 0.5886 - val_loss: 1.1741 - learning_rate: 1.0000e-05\n",
      "Training finished.\n",
      "Final validation accuracy: 59.03%\n"
     ]
    }
   ],
   "source": [
    "tl_history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_dataset),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "tl_final_val_accuracy = round(max(tl_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {tl_final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "tl_model_filename = 'InceptionV3TL' + str(tl_final_val_accuracy) + '.keras'\n",
    "model.save(tl_model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJfYvaTyenL9"
   },
   "source": [
    "## ğŸ”§ Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vd2tLbkueuTt"
   },
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model(tl_model_filename)\n",
    "\n",
    "# Set the model layers as trainable\n",
    "ft_model.get_layer('inception_v3').trainable = True\n",
    "\n",
    "# Set all layers as non-trainable\n",
    "for layer in ft_model.get_layer('inception_v3').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "t3N_PBk2gqiR"
   },
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 100\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "loss = tfk.losses.CategoricalCrossentropy()\n",
    "optimizer = tfk.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy']\n",
    "ft_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc_dOYkghI92"
   },
   "source": [
    "## ğŸ§  Train Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmWtYFf7hHLc",
    "outputId": "b72bc7fa-4182-4c06-c750-c71f5a834424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 68ms/step - accuracy: 0.1808 - loss: 3.1612 - val_accuracy: 0.1430 - val_loss: 2.0191 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 29ms/step - accuracy: 0.1768 - loss: 3.0374 - val_accuracy: 0.3754 - val_loss: 1.6470 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 30ms/step - accuracy: 0.2091 - loss: 2.8446 - val_accuracy: 0.4323 - val_loss: 1.5040 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 30ms/step - accuracy: 0.2230 - loss: 2.8217 - val_accuracy: 0.4172 - val_loss: 1.5921 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m598/598\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 30ms/step - accuracy: 0.2339 - loss: 2.7125 - val_accuracy: 0.4247 - val_loss: 1.5447 - learning_rate: 1.0000e-05\n",
      "Training finished.\n",
      "Final validation accuracy: 43.23%\n"
     ]
    }
   ],
   "source": [
    "ft_history = ft_model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_dataset),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "ft_final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {ft_final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file\n",
    "ft_model.save('weights.keras')\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YES9mYNvzdSo"
   },
   "source": [
    "## âœ… Verify that the weights work as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDrXpqfUzexe",
    "outputId": "bbc89587-5eec-47ad-d5f0-b095f87d4633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 152ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Validation Accuracy: 0.1430\n",
      "Test Accuracy: 0.1472\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = tfk.models.load_model('weights.keras')\n",
    "\n",
    "# Predict on test set and validation set\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Convert to class labels\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "y_pred_val_classes = np.argmax(y_pred_val, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "y_val_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "test_accuracy = np.sum(y_test_classes == y_pred_test_classes) / len(y_test_classes)\n",
    "val_accuracy = np.sum(y_val_classes == y_pred_val_classes) / len(y_val_classes)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## ğŸ“Š Create the model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKT4h-9xYwiT",
    "outputId": "f6290103-2948-4f93-d30f-0b38a4ef50a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.neural_network = tfk.models.load_model('weights.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CircvnFi4nX_"
   },
   "source": [
    "## ğŸ“ Export the ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s18kX1uDconq",
    "outputId": "f98ccb36-4980-473e-bd0a-76fff6e22aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.py (deflated 49%)\n",
      "  adding: weights.keras (deflated 8%)\n"
     ]
    }
   ],
   "source": [
    "# Set filename for the zip file\n",
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Create a zip file with the provided filename, containing model and weights\n",
    "!zip {filename} model.py weights.keras"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
